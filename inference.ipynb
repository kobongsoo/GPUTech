{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79822c4d-0cdb-463e-8bcf-fa37d653ade6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so.11.0'), PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "218eb1faf8b94d28b3ec702379ec749a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): GPTNeoXForCausalLM(\n",
      "      (gpt_neox): GPTNeoXModel(\n",
      "        (embed_in): Embedding(30080, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-27): 28 x GPTNeoXLayer(\n",
      "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "            (attention): GPTNeoXAttention(\n",
      "              (rotary_emb): RotaryEmbedding()\n",
      "              (query_key_value): Linear8bitLt(\n",
      "                in_features=4096, out_features=12288, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "            )\n",
      "            (mlp): GPTNeoXMLP(\n",
      "              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "              (act): GELUActivation()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (embed_out): Linear(in_features=4096, out_features=30080, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "time: 40565.75 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#=======================================================================\n",
    "# 예제 : sLLM을 이용한 chat봇 예제 \n",
    "#\n",
    "#=======================================================================\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "from myutils import GPU_info, seed_everything\n",
    "device = GPU_info()\n",
    "#seed 설정\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "# param 설정\n",
    "lora_weights:str = '../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B-LoRA-qa-2/'\n",
    "model_path:str ='../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'\n",
    "\n",
    "uselora_weight = True # Lora 사용하는 경우 True\n",
    "load_8bit = True\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained( model_path)\n",
    "\n",
    "# 원본 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(model_path, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "if uselora_weight:\n",
    "    model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16) # loRA 모델 로딩\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()\n",
    "    \n",
    "print(model.eval())\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be63990f-3e61-4d64-abe0-5e319bba3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "eos_str = tokenizer.decode(tokenizer.eos_token_id)\n",
    "\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "        \n",
    "def handle_query(max_new_tokens:int=256):\n",
    "    \n",
    "    query = input(\"질문:\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #prompt = query\n",
    "    #prompt = f\"<mocoquery>{query}</mocoquery>\\n\\n<answer>\"\n",
    "    #prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "    #prompt = f\"### 질문 : 간략히 답변해줘.{query}\\r\\n###답변:\"\n",
    "    prompt = f\"### 질문: {query}\\n\\n### 답변:\"\n",
    "    \n",
    "    #print(prompt)\n",
    "    \n",
    "    # config 설정\n",
    "    generation_config = GenerationConfig(\n",
    "            temperature=0.5,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            num_beams=1,\n",
    "            bos_token_id=tokenizer.bos_token_id,  # 시작토큰 \n",
    "            eos_token_id=tokenizer.eos_token_id,  # end 토큰\n",
    "            pad_token_id=tokenizer.pad_token_id   # padding 토큰\n",
    "    )\n",
    "    \n",
    "    # 프롬프트 tokenizer \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    #print(input_ids)\n",
    "\n",
    "    # Without streaming\n",
    "    # generate 처리\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    # 출력\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    print(output.replace(eos_str, ''))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039fbc0b-7dd2-46ad-b7ed-b10c8a62c723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문: 모코엠시스란\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 11229.52 ms\n",
      "\n",
      "### 질문: 모코엠시스란\n",
      "\n",
      "### 답변:모코엠시스는'mocha'라는 단어의 일본식 발음으로, '모카'라는 커피 원두에 '바나나'라는 과일을 섞어 만든 디저트입니다. 이는 60년대에 일본에서 유행하던 커피 + 바나나 조합으로 탄생했습니다. 이후, 전 세계적으로 유행하게 된 디저트이며, 특히 미국과 이탈리아에서 유명합니다. 모코엠시스는 초콜릿과 생크림을 위에 얹어 만든 달콤하고 부드러운 디저트입니다. \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문: 모코엠시스는 어떤 회사인가요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2254.89 ms\n",
      "\n",
      "### 질문: 모코엠시스는 어떤 회사인가요?\n",
      "\n",
      "### 답변:모코엠시스는 휴대용 무선 충전기를 개발하는 회사입니다.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "질문: 모코엠시스 업체는 어떤 일을 하나요?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 12356.82 ms\n",
      "\n",
      "### 질문: 모코엠시스 업체는 어떤 일을 하나요?\n",
      "\n",
      "### 답변:모코엠시스는 휴대용 유전자 분석 서비스를 제공하는 국내 유일의 유전자 분석 기업입니다. 모코엠시스는 유전자 분석을 통해 질병, 건강, 성격 등 다양한 분야에서 활용될 수 있는 기술을 개발하고 있습니다. 모코엠시스는 휴대용 유전자 분석 시스템을 구축하여, 이를 통해 다양한 질병, 건강, 성격 등을 분석하여 서비스를 제공하고 있습니다. 최근에는 코로나19 진단 키트에도 모코엠시스 기술이 활용되어 효과적인 결과를 내고 있습니다. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bde079-e2f9-4b43-9ac0-a4c871e7d8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
