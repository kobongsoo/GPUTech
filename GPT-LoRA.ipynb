{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/bloomz-7b1-mt-ft_2023-06-09.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# kakaobrain/kogpt 모델을 이용하여 PEFT(Parameter-Efficient Fine-tuning)->LoRA(Low-Rank Adaptation) 기법으로 Q&A 파인튜닝 하는 예시\n",
    "#\n",
    "# 참조 DOC: https://huggingface.co/docs/peft/index\n",
    "# 참조소스: https://github.com/huggingface/peft\n",
    "#\n",
    "# package 설치 \n",
    "# peft: pip install peft\n",
    "# load_in_8bit : pip install -i https://test.pypi.org/simple/ bitsandbytes-cudaXXX  (XXX는 CUDA version (e.g. 11.6 = 116))\n",
    "# transfomers 4.27.1 이상으로 업데이트 : pip install -U transformers[pytorch]\n",
    "# dispatch_model() got an unexpected keyword argument 'offload_index' 오류 => accelerate 업데이트 : pip install -U accelerate\n",
    "# module 'bitsandbytes.nn' has no attribute 'Linear8bitLt' => pip install bitsandbytes==0.37.2\n",
    "# 'MatmulLtState' object has no attribute 'memory_efficient_backward' 오류 => bitsandbytes 버전 0.37.2 설치 : pip install bitsandbytes==0.37.2\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "\n",
    "# 입력\n",
    "model_path='../data11/model/LLM/bigscience/bloomz-7b1-mt/'\n",
    "#model_path='bigscience/bloom-3b'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"bloomz-7b1-mt-ft\", logfilename=\"../log/bloomz-7b1-mt-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0154b5d0-18f0-4bea-82f0-6c2415c4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06240c79-1531-4eab-a43f-5c608cbaabbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91747409bf9a45509a01ad25ca80f32a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch import float32, nn, exp\n",
    "\n",
    "\n",
    "# 참고 소스 : https://github.com/jeremyarancio/llm-rpg/blob/main/llm/training_utils.py\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): \n",
    "        return super().forward(x).to(float32)\n",
    "\n",
    "    \n",
    "def prepare_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "            param.data = param.data.to(float32)\n",
    "    model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "    return model\n",
    "\n",
    "'''\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "'''\n",
    "'''\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path, revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True\n",
    ").to(device=device, non_blocking=True)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "'''\n",
    "\n",
    "# 모델 로딩\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, return_dict=True, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "918bd536-13e0-4dc5-aeb8-81e475013aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BloomForCausalLM(\n",
       "  (transformer): BloomModel(\n",
       "    (word_embeddings): Embedding(250880, 4096)\n",
       "    (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "    (h): ModuleList(\n",
       "      (0-29): 30 x BloomBlock(\n",
       "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (self_attention): BloomAttention(\n",
       "          (query_key_value): Linear8bitLt(in_features=4096, out_features=12288, bias=True)\n",
       "          (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): BloomMLP(\n",
       "          (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
       "          (gelu_impl): BloomGelu()\n",
       "          (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=250880, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e44db266-3ae7-4a58-a41e-b6212ca598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora 와 연동\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = prepare_model(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "#peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d22fc288-ce75-4db8-aea8-16465ad6afb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): BloomForCausalLM(\n",
       "      (transformer): BloomModel(\n",
       "        (word_embeddings): Embedding(250880, 4096)\n",
       "        (word_embeddings_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (h): ModuleList(\n",
       "          (0-29): 30 x BloomBlock(\n",
       "            (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (self_attention): BloomAttention(\n",
       "              (query_key_value): Linear8bitLt(\n",
       "                in_features=4096, out_features=12288, bias=True\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=16, out_features=12288, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
       "              (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): BloomMLP(\n",
       "              (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
       "              (gelu_impl): BloomGelu()\n",
       "              (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (lm_head): CastOutputToFloat(\n",
       "        (0): Linear(in_features=4096, out_features=250880, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2bc32f8-d7ce-4f57-ba92-2c3454335275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7864320 || all params: 7076880384 || trainable%: 0.11112693126452029\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "#print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ac49472-e1c8-4efb-af59-4dd405f64a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋을 생성한다.\n",
    "# => 문장뒤에 </s> 추가.\n",
    "from datasets import Dataset\n",
    "#corpus_path = \"../data11/korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "corpus_path = \"../data11/ai_hub/vs1/IT.과학.txt\"\n",
    "texts = \"\"\n",
    "with open(corpus_path, 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line) > 1:\n",
    "            texts += line\n",
    "        \n",
    "    # 문장들에서 '\\n'(띄어쓰기) -> '.</s>'로 치환함.\n",
    "    texts = texts.replace(\"\\n\", \".\" + tokenizer.eos_token)\n",
    " \n",
    "# 데이터 셋 생성\n",
    "dataset = Dataset.from_dict({'text': [texts]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32de01ef-69cf-4632-add9-29b5c960123d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d345699b3bb4c03bcf28d90053cba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized_dataset):10415\n",
      "len(tokenized_dataset_dict):2\n"
     ]
    }
   ],
   "source": [
    "from typing import Callable, Mapping\n",
    "\n",
    "# tokenizer 처리 함수\n",
    "def tokenize(element: Mapping, tokenizer: Callable, context_length: int) -> str:\n",
    "    inputs = tokenizer(element['text'], truncation=True, return_overflowing_tokens=True, \n",
    "                       return_length=True, max_length=context_length)\n",
    "    inputs_batch = []\n",
    "    for length, input_ids in zip(inputs['length'], inputs['input_ids']):\n",
    "        if length == context_length: # We drop the last input_ids that are shorter than max_length\n",
    "            inputs_batch.append(input_ids)\n",
    "    return {\"input_ids\": inputs_batch}\n",
    "\n",
    "# tokenizer dataset 생성\n",
    "context_length = 2048  # 문장 길이(모델의 허용할수 있는 최대 길이 설정)\n",
    "test_size = 0.001\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': tokenizer, 'context_length': context_length},\n",
    "                                         remove_columns=dataset.column_names)\n",
    "\n",
    "tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=test_size, shuffle=True)\n",
    "\n",
    "print(f'len(tokenized_dataset):{len(tokenized_dataset)}')\n",
    "print(f'len(tokenized_dataset_dict):{len(tokenized_dataset_dict)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1099a1eb-9f38-40d0-9813-3d571428548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 param 설정 \n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "output_dir = './output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=3e-4, \n",
    "    fp16=True,\n",
    "    logging_steps=1000,    # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset_dict['train'],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec23cf9-9cb7-4a56-8df7-6854c10bf02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkobongsoo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.14"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/MOCOMSYS/dev/bong/GPUTech/wandb/run-20230609_143714-1dmqoi3d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/kobongsoo/huggingface/runs/1dmqoi3d\" target=\"_blank\">peach-cloud-13</a></strong> to <a href=\"https://wandb.ai/kobongsoo/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BloomTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:298: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='299' max='31212' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  299/31212 26:44 < 46:23:47, 0.19 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "model.config.use_cache = False  # silence warnings\n",
    "trainer.train()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8291f86-a0ce-4b56-9ca6-db731b377146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../data11/model/LLM/bigscience/bloomz-7b1-mt-LoRA/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c125a1-4b4a-4bed-983c-152ee1a6304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06364ff-603f-46e9-9ca7-37cbda99f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576834b-126a-4209-b0c7-90da5d76a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Import the model\n",
    "hf_repo = '../data11/model/LLM/kakao/kogpt-float16-ft-last/'\n",
    "\n",
    "config = PeftConfig.from_pretrained(hf_repo)\n",
    "config.base_model_name_or_path = '../data11/model/LLM/kakao/kogpt-float16/'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Lora model 불러오기\n",
    "model = PeftModel.from_pretrained(model, hf_repo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f731de7-1c7e-47e9-8385-3e8abc5a00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28c0e3-0e6a-4b63-91eb-eca4b2695c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "수학은 기원전 600년 경에 살았던 탈레스로부터 시작됐다.\n",
    "하지만 탈레스가 태어나기 전에도 수학을 연구한 사람이 있을 수도 있기 때문에, 인류의 역사와 더불어 시작되었다고 할 수 있다.\n",
    "교역•분배•과세 등의 인류의 사회 생활에 필요한 모든 계산을 수학이 담당해 왔고, 농경 생활에 필수적인 천문 관측과 달력의 제정, 토지의 측량 또한 수학이 직접적으로 관여한 분야이다.\n",
    "고대 수학을 크게 발전시킨 나라로는 이집트, 인도, 그리스, 중국 등이 있다.\n",
    "그 중에서도 그리스는 처음으로 수학의 방정식에서 변수를 문자로 쓴 나라이다.\n",
    "\n",
    "위 내용을 바탕으로 아래 질문에 대해 답변해 주세요\n",
    "질문 : 고대 수학을 가장 크게 발전시킨 곳은?\n",
    "'''\n",
    "prompt =\"오늘 날씨는 비가 오고 있다. 내일 날씨는 \"\n",
    "\n",
    "## Generate\n",
    "max_new_tokens = 200\n",
    "temperature = 0.5\n",
    "do_sample = False\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            #eos_token_id=tokenizer.eos_token_id,\n",
    "            #early_stopping=True\n",
    "        )\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b063d8-a805-4b35-9d42-c1cfda27ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 평가 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6275bb2-f8d0-419d-9b70-496bc24d9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 로딩\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Import the model\n",
    "model_path = '../data11/model/LLM/kakao/kogpt-float16/'\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89fe57-0032-41a1-9a01-000aaec30447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8641133-bc3d-4982-ba5c-b136034c8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt1 =\"오늘 날씨는 비가 오고 있다. 내일 날씨는 \"\n",
    "\n",
    "## Generate\n",
    "max_new_tokens = 200\n",
    "temperature = 0.5\n",
    "do_sample = False\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer1(prompt1, return_tensors=\"pt\")\n",
    "\n",
    "tokens1 = model1.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            #eos_token_id=tokenizer.eos_token_id,\n",
    "            #early_stopping=True\n",
    "        )\n",
    "\n",
    "print(tokenizer1.decode(tokens1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109bb17-8ebf-4c4e-b0e4-7eb0e978f0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
