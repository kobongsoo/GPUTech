{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "device: cuda:0\n",
      "cuda index: 0\n",
      "gpu 개수: 1\n",
      "graphic name: NVIDIA A30\n",
      "cuda:0\n",
      "logfilepath:../log/quantumaikr-ft_2023-06-23.log\n"
     ]
    }
   ],
   "source": [
    "#====================================================================================================\n",
    "# loRA 파인튜닝 예제2\n",
    "# - quantumaikr/KoreanLM 모델을 이용하여 PEFT(Parameter-Efficient Fine-tuning)->LoRA(Low-Rank Adaptation) 기법으로 파인튜닝하는 예시\n",
    "#\n",
    "# 참조 DOC: https://github.com/quantumaikr/KoreanLM/tree/main\n",
    "# 참고소스: https://github.com/quantumaikr/KoreanLM/blob/main/finetune-lora.py\n",
    "# 참고소스 : https://github.com/quantumaikr/KoreanLM/blob/main/generate.py\n",
    "#\n",
    "# <package 설치> \n",
    "# peft: pip install peft\n",
    "# load_in_8bit : pip install -i https://test.pypi.org/simple/ bitsandbytes-cudaXXX  (XXX는 CUDA version (e.g. 11.6 = 116))\n",
    "# transfomers 4.27.1 이상으로 업데이트 : pip install -U transformers[pytorch]\n",
    "# dispatch_model() got an unexpected keyword argument 'offload_index' 오류 => accelerate 업데이트 : pip install -U accelerate\n",
    "# module 'bitsandbytes.nn' has no attribute 'Linear8bitLt' => pip install bitsandbytes==0.37.2\n",
    "# 'MatmulLtState' object has no attribute 'memory_efficient_backward' 오류 => bitsandbytes 버전 0.37.2 설치 : pip install bitsandbytes==0.37.2\n",
    "#====================================================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 입력 모델 경로\n",
    "model_path:str ='../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B'\n",
    "#model_path='beomi/KoRWKV-1.5B'\n",
    "\n",
    "# 훈련 관련 출력 경로\n",
    "out_dir:str = '../data11/model/LLM/out/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"quantumaikr-ft\", logfilename=\"../log/quantumaikr-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce454d45-bc97-4fd4-8305-737c540e3a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# KorQuAD_v1.0_train.json 혹은 KorQuAD_v1.0_dev.json 파일을 불러와서, sLLM 훈련을 위한 json 파일로 만듬.\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class QAExample:\n",
    "    # 질문 : 임종석이 여의도 농민 폭력 시위를 주도한 혐의로 지명수배 된 날은?\n",
    "    question_text: str\n",
    "    # (답 찾는 대상인)지문 : 1989년 2월 15일 여의도 농민 폭력 시위를 주도한 혐의 ... 서울지방경찰청 공안분실로 인계되었다.\n",
    "    context_text: str\n",
    "    # 답변 : 1989년 2월 15일\n",
    "    answer_text: str\n",
    "    \n",
    "corpus_fpath = '../data11/korpora/korQuAD/KorQuAD_v1.0_dev.json'\n",
    "examples = []\n",
    "         \n",
    "# KorQuAD_v1.0_train.json 파일을 불러옴\n",
    "json_data = json.load(open(corpus_fpath, \"r\", encoding=\"utf-8\"))[\"data\"]\n",
    "for entry in tqdm(json_data):\n",
    "        for paragraph in entry[\"paragraphs\"]:\n",
    "            context_text = paragraph[\"context\"]\n",
    "            for qa in paragraph[\"qas\"]:\n",
    "                question_text = qa[\"question\"]\n",
    "                for answer in qa[\"answers\"]:\n",
    "                    answer_text = answer[\"text\"]\n",
    "                    start_position_character = answer[\"answer_start\"]\n",
    "\n",
    "                    # question, context, answer, startposition 등을 설정함\n",
    "                    if question_text and answer_text and context_text and start_position_character:\n",
    "                        doc = {}\n",
    "                        doc['question'] = question_text\n",
    "                        doc['context'] = context_text\n",
    "                        doc['answer'] = answer_text\n",
    "                        \n",
    "                        examples.append(doc)\n",
    "                        \n",
    "docs ={}\n",
    "docs['text'] = examples\n",
    "\n",
    "#json 파일로 저장.\n",
    "# JSON 파일을 엽니다.\n",
    "with open(\"KorQuAD_v1.0_dev_data.json\", \"w\") as f:\n",
    "    # 리스트를 JSON으로 변환합니다.\n",
    "    json.dump(docs, f, indent=4)\n",
    "\n",
    "# JSON 파일을 닫습니다.\n",
    "f.close()\n",
    "\n",
    "# load_data로 불러와서 테스트 해봄.\n",
    "from datasets import load_dataset \n",
    "data = load_dataset(\"json\", data_files='KorQuAD_v1.0_dev_data.json', field=\"text\")\n",
    "\n",
    "print(data)\n",
    "print(data['train'][0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "766c02d5-1200-4e47-8b56-bad59761b910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so\n",
      "CUDA SETUP: CUDA runtime path found: /MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so'), PosixPath('/MOCOMSYS/anaconda3/envs/bong/lib/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff1b58ee9d454e62a53d40cdd074081c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPTNeoXForCausalLM(\n",
      "  (gpt_neox): GPTNeoXModel(\n",
      "    (embed_in): Embedding(30080, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-27): 28 x GPTNeoXLayer(\n",
      "        (input_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (post_attention_layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "        (attention): GPTNeoXAttention(\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "          (query_key_value): Linear8bitLt(in_features=4096, out_features=12288, bias=True)\n",
      "          (dense): Linear8bitLt(in_features=4096, out_features=4096, bias=True)\n",
      "        )\n",
      "        (mlp): GPTNeoXMLP(\n",
      "          (dense_h_to_4h): Linear8bitLt(in_features=4096, out_features=16384, bias=True)\n",
      "          (dense_4h_to_h): Linear8bitLt(in_features=16384, out_features=4096, bias=True)\n",
      "          (act): GELUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (embed_out): Linear(in_features=4096, out_features=30080, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# CausalLM 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        load_in_8bit=True,           # 8bit 양자화.\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        #cache_dir=\"./cache/\",\n",
    "    )\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        #cache_dir=\"./cache/\",\n",
    "    )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c5890df-dac1-4db6-bc39-578e1105fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 설정 \n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# int8 양자화 처리를 위해, 전처리 함.\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# **Polyglot 모델인 경우에만 주석처리\n",
    "'''\n",
    "# loRA config 설정\n",
    "lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "'''\n",
    "config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        #target_modules=lora_target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# CausalLM 모델과 loRA 모델 연동\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc5eae6c-9294-426c-be6b-88dc1792be98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3670016 || all params: 5888729088 || trainable%: 0.06232271760435925\n"
     ]
    }
   ],
   "source": [
    "# LoRA 훈련할 파라메터 계수 출력 해봄\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b92becf-35d0-424b-98f2-d5a941953909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "prompt 테스트----------------------\n",
      "\n",
      "### 지시: [질의]에 가장 적합한 [응답]을, 제시된 [문단]에서 찾아서 간단한 문장으로 답변해주세요.\n",
      "\n",
      "### [질의]: 제주도의 크기는 얼마?\n",
      "\n",
      "n### [문단]: 제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.\n",
      "\n",
      "### [응답]:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-565617c296da1bf1\n",
      "Reusing dataset json (/MOCOMSYS/.cache/huggingface/datasets/json/default-565617c296da1bf1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a096bae09e22459bb24f5972bd28b8b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /MOCOMSYS/.cache/huggingface/datasets/json/default-565617c296da1bf1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-f66fba475399cd26.arrow and /MOCOMSYS/.cache/huggingface/datasets/json/default-565617c296da1bf1/0.0.0/ac0ca5f5289a6cf108e706efcf040422dbbfa8e658dee6a819f20d76bb84d26b/cache-dd357a582227dfc3.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f377dd723c4b68a75703fe266abef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300bb63af93c4a77950dfdf95a9f0535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 말뭉치 로딩\n",
    "import os.path as osp\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# param---------------------------------------------------\n",
    "data_path='./data/KorQuAD_v1.0_dev_data.json'  # 말뭉치 파일경로\n",
    "val_set_size = 100                # 평가 말뭉치 사이즈   \n",
    "cutoff_len=1024                   # 토크너나이저 길이\n",
    "add_eos_token=True               # True=말뭉치 뒤에 add_eos_token(</s>) 추가함 \n",
    "shuffle = True                   # True=말뭉치를 랜덤하게 섞은다.\n",
    "\n",
    "# prompt_template(모델에 따라 변경)\n",
    "PROMPT_DICT = {    \n",
    "    \"prompt_context\":(\"### 지시: [질의]에 가장 적합한 [응답]을, 제시된 [문단]에서 찾아서 간단한 문장으로 답변해주세요.\\n\\n### [질의]: {query}\\n\\nn### [문단]: {context}\\n\\n### [응답]:\"),\n",
    "    \"prompt_no_context\":(\"### 지시: 질의에 대해, 간단한 문장으로 응답해 주세요 ### [질의]: {query}\\n\\n### [응답]:\")\n",
    "}\n",
    "\n",
    "# prompt 테스트 \n",
    "print(f'\\n\\nprompt 테스트----------------------\\n')\n",
    "query='제주도의 크기는 얼마?'\n",
    "#context = ''\n",
    "context = '제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.'\n",
    "if context:\n",
    "    prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "else:\n",
    "    prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "    \n",
    "print(prompt)\n",
    "\n",
    "#--------------------------------------------------------\n",
    "\n",
    "# 실제 입력 말뭉치를 tokenizer 하고, label고 만드는 과정\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # eos 토큰을 추가함.\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    # label은 input_ids 복사해서 만듬.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "#---------------------------------------------------\n",
    "# 말뭉치 종류에 땨라, 훈련 데이터 생성\n",
    "# - 말뭉치 종류에 따라 프롬프트 템블릿에 맞추어 수정해야 함.\n",
    "#---------------------------------------------------\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "\n",
    "    # templeate 로딩함\n",
    "    #description:\"KoreanLM-LoRA에서 사용하는 템플릿입니다.\"\n",
    "    #prompt_input:\"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Input: {input} ### Response: \"\n",
    "    #prompt_no_input:\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Response: \"\n",
    "    #response_split:\"### Response:\"\n",
    "    '''\n",
    "    file_name = \"./data/templates/korean.json\"\n",
    "    #file_name = \"./data/templates/qanda.json\" # 말뭉치에 맞게 템플릿을 변경해야함\n",
    "    if not osp.exists(file_name):\n",
    "        raise ValueError(f\"Can't read {file_name}\")\n",
    "            \n",
    "    with open(file_name) as fp:\n",
    "        PROMPT_DICT = json.load(fp)\n",
    "    '''\n",
    "    '''\n",
    "    # input 있으면 instruction+input 시킴\n",
    "    if data_point[\"input\"]:\n",
    "        res = PROMPT_DICT[\"prompt_context\"].format(query=data_point[\"instruction\"], context=data_point[\"input\"])\n",
    "    else:\n",
    "        res = PROMPT_DICT[\"prompt_no_context\"].format(query=data_point[\"instruction\"])    \n",
    "    \n",
    "\n",
    "     # output있으면 +output 시켜줌.\n",
    "    if data_point[\"output\"]:\n",
    "        full_prompt = f'{res}{data_point[\"output\"]}'\n",
    "    else:\n",
    "        full_prompt = res\n",
    "    '''\n",
    "  \n",
    "    # context 있으면 instruction+input 시킴\n",
    "    if data_point[\"context\"]:\n",
    "        res = PROMPT_DICT[\"prompt_context\"].format(query=data_point[\"question\"], context=data_point[\"context\"])\n",
    "    else:\n",
    "        res = PROMPT_DICT[\"prompt_no_context\"].format(query=data_point[\"question\"])    \n",
    "   \n",
    "     # output있으면 +output 시켜줌.\n",
    "    if data_point[\"answer\"]:\n",
    "        full_prompt = f'{res}{data_point[\"answer\"]}'\n",
    "    else:\n",
    "        full_prompt = res\n",
    "     \n",
    "    # tokenizer 처리\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    \n",
    "    return tokenized_full_prompt\n",
    "#---------------------------------------------------\n",
    "\n",
    "\n",
    "# 말뭉치 로딩\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    #----------------------------------------------------------\n",
    "    # {\n",
    "    #     text[\n",
    "    #         {},\n",
    "    #         {},\n",
    "    #         {}\n",
    "    #     ]\n",
    "    # } \n",
    "    # 식으로 json 파일 구조가 되어 있는 파일이어야 함.\n",
    "    #----------------------------------------------------------\n",
    "    data = load_dataset(\"json\", data_files=data_path, field=\"text\")\n",
    "else:\n",
    "    data = load_dataset(data_path)     \n",
    "    \n",
    "\n",
    "# train, test 말뭉치로 분할\n",
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=shuffle, seed=SEED\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93726091-9f13-4b6b-aca0-5501960b5508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': '1943년에 일본의 강요에 의해 조선기독교단으로 강제 통합당한 교회는? ', 'context': '이후 일본의 강요로 1943년 조선기독교단으로 강제 통합당한 감리교회는, 해방후 다시 분리되어 새로 교단을 구성하는 과정에서 재건파와 복흥파로 분리되는 등 홍역을 앓았다. 그러나 그 이후 다시 통합되어 1949년 현재의 이름이 채택되었다. 이후 1971년 경기연회 사건으로 총리원과 총회(갱신)측으로 분리되기도 하였으나[3], 이후 재통합돼 감리교단은 현재 대한민국 개신교 교파중에서 유일하게 교파 분열을 겪지 않은 교회이다. 다만 2008년 감독회장 선거로 시작된 기독교대한감리회 감독회장 선출 논쟁이 있었으나 2013년 현재 전용재 목사가 감독회장에 당선됨으로써 일단락됐다. 현재까지 감독 선출과 교회 운영은 감리회 본부와 총회를 중심으로 이루어지고 있다.', 'answer': '감리교회', 'input_ids': [6, 6, 6, 3634, 29, 5485, 796, 285, 64, 274, 1063, 5527, 288, 5485, 9643, 64, 276, 15, 2154, 551, 5485, 16468, 64, 363, 1892, 306, 4786, 288, 7990, 339, 4253, 29143, 1934, 17, 202, 202, 6, 6, 6, 5485, 796, 285, 64, 29, 1024, 5675, 440, 274, 1191, 285, 7177, 274, 3700, 2141, 21424, 539, 339, 4002, 2047, 9616, 5278, 272, 34, 224, 202, 202, 81, 6, 6, 6, 5485, 16468, 64, 29, 1071, 1191, 285, 7177, 286, 1024, 5675, 440, 2141, 21424, 539, 339, 4002, 2047, 9616, 20080, 5764, 272, 15, 9257, 694, 1208, 4598, 605, 348, 3511, 19459, 276, 1544, 284, 272, 1458, 363, 15264, 812, 441, 1149, 1713, 812, 286, 4598, 605, 272, 433, 21774, 276, 6348, 622, 267, 17, 1252, 353, 1071, 1208, 2047, 605, 348, 1024, 4709, 440, 1167, 285, 1756, 270, 4904, 605, 450, 267, 17, 1071, 21884, 440, 1015, 583, 437, 1514, 339, 2473, 352, 359, 7887, 11, 24959, 12, 1524, 339, 4598, 605, 15596, 423, 12052, 62, 22, 64, 15, 1071, 706, 3289, 1077, 20080, 594, 539, 296, 1167, 2775, 24486, 665, 812, 653, 363, 3141, 284, 379, 665, 812, 8756, 276, 2183, 295, 536, 296, 5278, 270, 267, 17, 2466, 2881, 440, 1468, 2100, 1633, 286, 1016, 551, 9929, 4965, 29714, 437, 1468, 2100, 5109, 7746, 270, 327, 450, 1620, 2878, 440, 1167, 2947, 558, 6238, 293, 1468, 2100, 274, 2807, 5579, 2848, 24017, 799, 267, 17, 1167, 617, 1468, 5109, 359, 5278, 1163, 296, 20080, 437, 5155, 441, 7887, 301, 1531, 339, 8295, 283, 327, 267, 17, 202, 202, 6, 6, 6, 5485, 9643, 64, 29, 29714, 5764, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [6, 6, 6, 3634, 29, 5485, 796, 285, 64, 274, 1063, 5527, 288, 5485, 9643, 64, 276, 15, 2154, 551, 5485, 16468, 64, 363, 1892, 306, 4786, 288, 7990, 339, 4253, 29143, 1934, 17, 202, 202, 6, 6, 6, 5485, 796, 285, 64, 29, 1024, 5675, 440, 274, 1191, 285, 7177, 274, 3700, 2141, 21424, 539, 339, 4002, 2047, 9616, 5278, 272, 34, 224, 202, 202, 81, 6, 6, 6, 5485, 16468, 64, 29, 1071, 1191, 285, 7177, 286, 1024, 5675, 440, 2141, 21424, 539, 339, 4002, 2047, 9616, 20080, 5764, 272, 15, 9257, 694, 1208, 4598, 605, 348, 3511, 19459, 276, 1544, 284, 272, 1458, 363, 15264, 812, 441, 1149, 1713, 812, 286, 4598, 605, 272, 433, 21774, 276, 6348, 622, 267, 17, 1252, 353, 1071, 1208, 2047, 605, 348, 1024, 4709, 440, 1167, 285, 1756, 270, 4904, 605, 450, 267, 17, 1071, 21884, 440, 1015, 583, 437, 1514, 339, 2473, 352, 359, 7887, 11, 24959, 12, 1524, 339, 4598, 605, 15596, 423, 12052, 62, 22, 64, 15, 1071, 706, 3289, 1077, 20080, 594, 539, 296, 1167, 2775, 24486, 665, 812, 653, 363, 3141, 284, 379, 665, 812, 8756, 276, 2183, 295, 536, 296, 5278, 270, 267, 17, 2466, 2881, 440, 1468, 2100, 1633, 286, 1016, 551, 9929, 4965, 29714, 437, 1468, 2100, 5109, 7746, 270, 327, 450, 1620, 2878, 440, 1167, 2947, 558, 6238, 293, 1468, 2100, 274, 2807, 5579, 2848, 24017, 799, 267, 17, 1167, 617, 1468, 5109, 359, 5278, 1163, 296, 20080, 437, 5155, 441, 7887, 301, 1531, 339, 8295, 283, 327, 267, 17, 202, 202, 6, 6, 6, 5485, 9643, 64, 29, 29714, 5764, 2]}\n",
      "\n",
      "len:\n",
      "270\n",
      "### 지시: [질의]에 가장 적합한 [응답]을, 제시된 [문단]에서 찾아서 간단한 문장으로 답변해주세요.\n",
      "\n",
      "### [질의]: 1943년에 일본의 강요에 의해 조선기독교단으로 강제 통합당한 교회는? \n",
      "\n",
      "n### [문단]: 이후 일본의 강요로 1943년 조선기독교단으로 강제 통합당한 감리교회는, 해방후 다시 분리되어 새로 교단을 구성하는 과정에서 재건파와 복흥파로 분리되는 등 홍역을 앓았다. 그러나 그 이후 다시 통합되어 1949년 현재의 이름이 채택되었다. 이후 1971년 경기연회 사건으로 총리원과 총회(갱신)측으로 분리되기도 하였으나[3], 이후 재통합돼 감리교단은 현재 대한민국 개신교 교파중에서 유일하게 교파 분열을 겪지 않은 교회이다. 다만 2008년 감독회장 선거로 시작된 기독교대한감리회 감독회장 선출 논쟁이 있었으나 2013년 현재 전용재 목사가 감독회장에 당선됨으로써 일단락됐다. 현재까지 감독 선출과 교회 운영은 감리회 본부와 총회를 중심으로 이루어지고 있다.\n",
      "\n",
      "### [응답]:감리교회<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "print(train_data[1])\n",
    "print()\n",
    "print('len:')\n",
    "print(len(train_data[1]['input_ids']))\n",
    "print(tokenizer.decode(train_data[1]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a3a2c1-17db-4430-bfef-59aa76d76706",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
     ]
    }
   ],
   "source": [
    "# 모델 훈련 param 설정\n",
    "micro_batch_size:int = 4\n",
    "batch_size:int = 128\n",
    "gradient_accumulation_steps:int = int(batch_size // micro_batch_size)\n",
    "num_epochs:int = 10\n",
    "learning_rate: float = 3e-4\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"epoch\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            #eval_steps=200 if val_set_size > 0 else None,\n",
    "            #save_steps=200,\n",
    "            output_dir=out_dir,\n",
    "            save_total_limit=2,\n",
    "            #load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            #ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=False,\n",
    "            #report_to=\"wandb\" if use_wandb else None,\n",
    "            #run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "    \n",
    "        # DataCollatorForSeq2Seq 로 지정.\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ee478-979d-4880-b008-f136496e018a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='120' max='420' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [120/420 3:55:21 < 9:58:22, 0.01 it/s, Epoch 2.80/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3.492000</td>\n",
       "      <td>3.076922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.388300</td>\n",
       "      <td>2.302826</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/MOCOMSYS/anaconda3/envs/bong/lib/python3.9/site-packages/bitsandbytes/autograd/_functions.py:321: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80d9ad-7ce6-4bc5-9d7e-21e944ed4697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 완료후 저장\n",
    "import os\n",
    "\n",
    "OUT_PATH = '../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B-LoRA-qa-3/'\n",
    "os.makedirs(OUT_PATH, exist_ok=True) # 폴더 생성\n",
    "\n",
    "# adapter_config.json 와 adapter_config.bin 파일 생성. \n",
    "model.save_pretrained(OUT_PATH)\n",
    " \n",
    "# 실제 모델 파일 (pytorch_model.bin, training_args.bin)\n",
    "trainer.save_state()  # 모델이 모든 가중치, 학습률, 알고리즘등을 저장.\n",
    "trainer.save_model(output_dir=OUT_PATH) # 모델이 가중치만 저장\n",
    "\n",
    "# pytorch_model.bin 이름을 adapter_model.bin 변경.\n",
    "# => 그래야 huggingface 를 이용해서 불러올수 있음.\n",
    "if os.path.exists(os.path.join(OUT_PATH, 'adapter_model.bin')):\n",
    "    os.remove(os.path.join(OUT_PATH, 'adapter_model.bin'))\n",
    "    print(f'remove adapter_model.bin')\n",
    "\n",
    "if os.path.exists(os.path.join(OUT_PATH, 'pytorch_model.bin')):\n",
    "    os.rename(os.path.join(OUT_PATH, 'pytorch_model.bin'), os.path.join(OUT_PATH, 'adapter_model.bin'))\n",
    "    print(f'rename pytorch_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab90451c-76ce-4bdc-b2b1-f0de79fcde30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c125a1-4b4a-4bed-983c-152ee1a6304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임.\n",
    "# 참고 소스 : https://github.com/quantumaikr/KoreanLM/blob/main/generate.py\n",
    "#\n",
    "# 1. prompt는 모델마다 훈련 프롴프트가 다르므로, 해당 모델 prompt 로 입력하는게 보나 나은 출력이 나옴.\n",
    "# => KoAlpaca, KoAlpaca-Polyglot-5.8B prompt 예시=> '### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:'\n",
    "#\n",
    "# => open_llama_7b, KoreanLM, KoreanLM-hf prompt 예시(data/korean.json 파일 참조)\n",
    "#  \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Input: {input} ### Response: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06364ff-603f-46e9-9ca7-37cbda99f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ec814-d031-44a8-bf54-45450abf7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "from myutils import GPU_info\n",
    "device = GPU_info()\n",
    "    \n",
    "#====================================================================\n",
    "# param 설정\n",
    "lora_weights:str = '../data11/model/LLM/beomin/KoAlpaca-Polyglot-5.8B/'\n",
    "model_path:str ='../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'\n",
    "\n",
    "uselora_weight = False # Lora 사용하는 경우 True\n",
    "load_8bit = False\n",
    "usekoalphaca = True   # Koalpaca 모델인 경우엔 True 해줘야함.\n",
    "#====================================================================\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# 원본 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=load_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "if uselora_weight:\n",
    "    # loRA 모델 로딩\n",
    "    model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()\n",
    "    \n",
    "model.eval()\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c35db-0d10-4dd7-9ff7-c8a63d3db343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Union\n",
    "import json\n",
    "from transformers import GenerationConfig\n",
    "#print(device)\n",
    "\n",
    "# open_llama_7b, KoreanLM, KoreanLM-hf prompt 템플릿 경로\n",
    "template_file_name = \"./data/templates/korean.json\"\n",
    "\n",
    "# open_llama_7b, KoreanLM, KoreanLM-hf prompt 만드는 함수.\n",
    "def generate_prompt(\n",
    "        instruction: str,               # 설명\n",
    "        input: Union[None, str] = None, # 입력:옵션\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "    \n",
    "    \n",
    "    if not osp.exists(template_file_name):\n",
    "        raise ValueError(f\"Can't read {template_file_name}\")\n",
    "            \n",
    "    with open(template_file_name) as fp:\n",
    "        template = json.load(fp)\n",
    "        \n",
    "    # returns the full prompt from instruction and optional input\n",
    "    # if a label (=response, =output) is provided, it's also appended.\n",
    "    if input:\n",
    "        res = template[\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "        \n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    \n",
    "    return res\n",
    "   \n",
    "'''\n",
    "대한민국의 남서쪽에 있는 섬으로 행정구역 상 광역자치단체인 제주특별자치도에 속한다. \n",
    "한국의 섬 중에서 가장 크고 인구가 많은 섬이기도 하며 면적은 1,833.2㎢이다. \n",
    "이는 대한민국 본토에서 가장 큰 기초자치단체인 홍천군(1,820.14㎢)보다 약간 크며, 제주도 다음 두 번째로 큰 섬인 거제도(379.5㎢)의 5배 정도 된다. \n",
    "인구는 약 70만 명, 세계 섬 크기 218위이다.\n",
    "제주도는 동아시아권 전체로 범위를 넓혀도 꽤 큰 섬에 속한다. \n",
    "6,000개가 넘는 섬이 있는 일본조차도 본토로 간주되는 혼슈, 홋카이도, 시코쿠, 규슈 4개 섬을 제외한 나머지 모든 섬이 제주도보다 작다.\n",
    "중국도 하이난 섬 한 곳만이 제주도보다 클 뿐이다.\n",
    "하와이에서도 최대 섬인 하와이 섬 다음으로 큰 섬인 마우이 섬이 제주도보다 약간 큰 정도이다. \n",
    "미국도 본토만 따지면 제주도보다 큰 섬은 롱아일랜드 뿐이다. \n",
    "프랑스도 본토에는 제주도보다 큰 섬이 코르시카 섬밖에 없고, 독일에서 가장 큰 섬인 뤼겐 섬은 제주도보다 작다. \n",
    "크기에 대한 직접적인 비교를 하자면 제주도의 동서 길이 약 73km, \n",
    "남 길이 약 31km를 대입하여 서울시청 기준 동서 길이로 인천광역시 서구 오류동 거첨도에서 출발하여 \n",
    "경기도 양평군 서종면에 도달하고 남북 길이로는 송추계곡에서 출발하여 관악산에 이르는 수준이다.\n",
    "\n",
    "질문 : 제주도 길이는 얼마?\n",
    "'''\n",
    "\n",
    "instruction ='''\n",
    "아래 내용을 가지고 질문에 간단히 답변해 주세요\n",
    "\n",
    "학자금 지원\n",
    "목적 : 임직원 자녀의 학자금을 지급함으로써 임직원의 복지 향상, 근로의욕의 제고 및 장기근속 유도\n",
    "지원 기준\n",
    "임직원의 고등학교 재학중인 자녀 학자금 지원\n",
    "고등학교 교육비(입학금, 수업료, 학교운영지원금, 교과서)\n",
    "자녀수 제한은 없으며, 자녀당 연간 300만원 한도에서 지원\n",
    "지급신청 : 학자금의 신청은 아래 신청시기 해당월 이내에 다음 각호의 신청서류를 구비하여 해당부서장 결재 득 후 경영지원본부에 신청\n",
    "학자금 신청서 (별첨1)\n",
    "납입고지서 영수증 사본\n",
    "취학자녀의 재학증명서(최초 신청시 1회에 한함)\n",
    "가족관계증명원(최초 신청시 1회에 한함)\n",
    "신청시기 : 납부 후 1개월 이내\n",
    "지급제외 대상\n",
    "휴학, 퇴학 등의 사유로 학적이 변동된 경우\n",
    "타기관, 단체, 학교에서 장학금을 받는 경우 (일부를 받는 경우는 차액 지급)\n",
    "취학자녀의 부모가 임직원으로 동시에 지급대상일 경우 1인만 적용\n",
    "기타 : 해당금액은 과세대상임\n",
    "'''\n",
    "\n",
    "#instruction = \"엠파워 이지스-씨 제품 설명\"\n",
    "\n",
    "input_text = '''\n",
    "학자금 신청 기준은 어떻게 되나?\n",
    "'''\n",
    "# prompt 구성\n",
    "#========================================================================\n",
    "# koAlphaca일때 prompt 구성.\n",
    "# => \"### 질문: {input}\\n\\n### 맥락: {context}\\n\\n### 답변:\" 식으로 입력. \n",
    "# => '### 맥락' 은 이전 대화 맥락임.\n",
    "#=========================================================================\n",
    "\n",
    "if usekoalphaca:\n",
    "    input_text = instruction+'\\r\\n'+input_text\n",
    "    context = \"\" #이전 대화 맥락 입력\n",
    "    \n",
    "    prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "else:\n",
    "    prompt = generate_prompt(instruction=instruction, input=input_text)\n",
    "\n",
    "max_new_tokens = 256\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# config 설정\n",
    "generation_config = GenerationConfig(\n",
    "            temperature=0.5,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# 프롬프트 tokenizer \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "#print(input_ids)\n",
    "\n",
    "# Without streaming\n",
    "# generate 처리\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=False,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    \n",
    "# 출력\n",
    "#print(generation_output)\n",
    "#print()\n",
    "\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)\n",
    "\n",
    "end_time = time.time() - start_time\n",
    "print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b063d8-a805-4b35-9d42-c1cfda27ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 평가 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109bb17-8ebf-4c4e-b0e4-7eb0e978f0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
