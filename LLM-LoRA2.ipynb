{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================\n",
    "# loRA 파인튜닝 예제2\n",
    "# - quantumaikr/KoreanLM 모델을 이용하여 PEFT(Parameter-Efficient Fine-tuning)->LoRA(Low-Rank Adaptation) 기법으로 파인튜닝하는 예시\n",
    "#\n",
    "# 참조 DOC: https://github.com/quantumaikr/KoreanLM/tree/main\n",
    "# 참고소스: https://github.com/quantumaikr/KoreanLM/blob/main/finetune-lora.py\n",
    "# 참고소스 : https://github.com/quantumaikr/KoreanLM/blob/main/generate.py\n",
    "#\n",
    "# <package 설치> \n",
    "# peft: pip install peft\n",
    "# load_in_8bit : pip install -i https://test.pypi.org/simple/ bitsandbytes-cudaXXX  (XXX는 CUDA version (e.g. 11.6 = 116))\n",
    "# transfomers 4.27.1 이상으로 업데이트 : pip install -U transformers[pytorch]\n",
    "# dispatch_model() got an unexpected keyword argument 'offload_index' 오류 => accelerate 업데이트 : pip install -U accelerate\n",
    "# module 'bitsandbytes.nn' has no attribute 'Linear8bitLt' => pip install bitsandbytes==0.37.2\n",
    "# 'MatmulLtState' object has no attribute 'memory_efficient_backward' 오류 => bitsandbytes 버전 0.37.2 설치 : pip install bitsandbytes==0.37.2\n",
    "#====================================================================================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "\n",
    "# wand 비활성화 \n",
    "# => trainer 로 훈련시키면 기본이 wandb 활성화이므로, 비활성화 시킴\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "# 입력 모델 경로\n",
    "model_path:str ='../data11/model/LLM/quantumaikr/KoreanLM-hf/'\n",
    "#model_path='quantumaikr/KoreanLM'\n",
    "\n",
    "# 훈련 관련 출력 경로\n",
    "out_dir:str = '../data11/model/LLM/quantumaikr/out/'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"quantumaikr-ft\", logfilename=\"../log/quantumaikr-ft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766c02d5-1200-4e47-8b56-bad59761b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CausalLM 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        load_in_8bit=True,           # 8bit 양자화.\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        #cache_dir=\"./cache/\",\n",
    "    )\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_path,\n",
    "        padding_side=\"right\",\n",
    "        use_fast=False,\n",
    "        #cache_dir=\"./cache/\",\n",
    "    )\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cbdb57-c608-467a-8579-0c8956747d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 출력\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5890df-dac1-4db6-bc39-578e1105fdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 설정 \n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    "    set_peft_model_state_dict,\n",
    ")\n",
    "\n",
    "# int8 양자화 처리를 위해, 전처리 함.\n",
    "model = prepare_model_for_int8_training(model)\n",
    "\n",
    "# loRA config 설정\n",
    "lora_target_modules: List[str] = [\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "    ]\n",
    "\n",
    "config = LoraConfig(\n",
    "        r=8,\n",
    "        lora_alpha=16,\n",
    "        target_modules=lora_target_modules,\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# CausalLM 모델과 loRA 모델 연동\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5eae6c-9294-426c-be6b-88dc1792be98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 훈련할 파라메터 계수 출력 해봄\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b92becf-35d0-424b-98f2-d5a941953909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 말뭉치 로딩\n",
    "import os.path as osp\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "data_path='./data/KoAlpaca_v1.1.1.json'  # 말뭉치 파일경로\n",
    "val_set_size = 10                # 평가 말뭉치 사이즈   \n",
    "cutoff_len=1024                   # 토크너나이저 길이\n",
    "add_eos_token=False             # True=말뭉치 뒤에 add_eos_token(</s>) 추가함 \n",
    "shuffle = True                 # True=말뭉치를 랜덤하게 섞은다.\n",
    "\n",
    "# 실제 입력 말뭉치를 tokenizer 하고, label고 만드는 과정\n",
    "def tokenize(prompt, add_eos_token=True):\n",
    "    # there's probably a way to do this with the tokenizer settings\n",
    "    # but again, gotta move fast\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=cutoff_len,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "    )\n",
    "    \n",
    "    # eos 토큰을 추가함.\n",
    "    if (\n",
    "        result[\"input_ids\"][-1] != tokenizer.eos_token_id\n",
    "        and len(result[\"input_ids\"]) < cutoff_len\n",
    "        and add_eos_token\n",
    "    ):\n",
    "        result[\"input_ids\"].append(tokenizer.eos_token_id)\n",
    "        result[\"attention_mask\"].append(1)\n",
    "\n",
    "    # label은 input_ids 복사해서 만듬.\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "\n",
    "    return result\n",
    "\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "\n",
    "    # templeate 로딩함\n",
    "    #description:\"KoreanLM-LoRA에서 사용하는 템플릿입니다.\"\n",
    "    #prompt_input:\"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Input: {input} ### Response: \"\n",
    "    #prompt_no_input:\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Response: \"\n",
    "    #response_split:\"### Response:\"\n",
    "    \n",
    "    file_name = \"./data/templates/korean.json\"\n",
    "    if not osp.exists(file_name):\n",
    "        raise ValueError(f\"Can't read {file_name}\")\n",
    "            \n",
    "    with open(file_name) as fp:\n",
    "        template = json.load(fp)\n",
    "             \n",
    "    # input 있으면 instruction+input 시킴\n",
    "    if data_point[\"input\"]:\n",
    "        res = template[\"prompt_input\"].format(instruction=data_point[\"instruction\"], input=data_point[\"input\"])\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(instruction=data_point[\"instruction\"])    \n",
    "\n",
    "    #res = template[\"prompt_no_input\"].format(instruction=data_point[\"instruction\"])\n",
    "\n",
    "     # output있으면 +output 시켜줌.\n",
    "    if data_point[\"output\"]:\n",
    "        full_prompt = f'{res}{data_point[\"output\"]}'\n",
    "    else:\n",
    "        full_prompt = res\n",
    "            \n",
    "    # tokenizer 처리\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    \n",
    "    return tokenized_full_prompt\n",
    "    \n",
    "# 입력 말뭉치 문장을 합쳐서, tokenizer 함\n",
    "# => 말뭉치가 변경되면 이 함수를 변경하면 됨.\n",
    "def generate_and_tokenize(data_point):\n",
    "    \n",
    "    # input 있으면 instruction+input 시킴\n",
    "    if data_point[\"input\"]:\n",
    "        full_prompt = f'{data_point[\"instruction\"]}{data_point[\"input\"]}'\n",
    "    else:\n",
    "        full_prompt = f'{data_point[\"instruction\"]}'\n",
    "    \n",
    "    #full_prompt = f'{data_point[\"instruction\"]}'\n",
    "    \n",
    "    # output있으면 +output 시켜줌.\n",
    "    if data_point[\"output\"]:\n",
    "        full_prompt = f'{full_prompt}{data_point[\"output\"]}'\n",
    "    \n",
    "    # tokenizer 처리\n",
    "    tokenized_full_prompt = tokenize(full_prompt)\n",
    "    \n",
    "    return tokenized_full_prompt\n",
    "\n",
    "\n",
    "# 말뭉치 로딩\n",
    "if data_path.endswith(\".json\") or data_path.endswith(\".jsonl\"):\n",
    "    #----------------------------------------------------------\n",
    "    # {\n",
    "    #     text[\n",
    "    #         {},\n",
    "    #         {},\n",
    "    #         {}\n",
    "    #     ]\n",
    "    # } \n",
    "    # 식으로 json 파일 구조가 되어 있는 파일이어야 함.\n",
    "    #----------------------------------------------------------\n",
    "    data = load_dataset(\"json\", data_files=data_path, field=\"text\")\n",
    "else:\n",
    "    data = load_dataset(data_path)     \n",
    "    \n",
    "\n",
    "# train, test 말뭉치로 분할\n",
    "if val_set_size > 0:\n",
    "    train_val = data[\"train\"].train_test_split(\n",
    "        test_size=val_set_size, shuffle=shuffle, seed=SEED\n",
    "    )\n",
    "    train_data = (\n",
    "        train_val[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "    val_data = (\n",
    "        train_val[\"test\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    )\n",
    "else:\n",
    "    train_data = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)\n",
    "    val_data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93726091-9f13-4b6b-aca0-5501960b5508",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])\n",
    "print()\n",
    "print('len:')\n",
    "print(len(train_data[0]['input_ids']))\n",
    "print(tokenizer.decode(train_data[0]['input_ids']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a3a2c1-17db-4430-bfef-59aa76d76706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 param 설정\n",
    "micro_batch_size:int = 4\n",
    "batch_size:int = 128\n",
    "gradient_accumulation_steps:int = int(batch_size // micro_batch_size)\n",
    "num_epochs:int = 5\n",
    "learning_rate: float = 3e-4\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "        model=model,\n",
    "        train_dataset=train_data,\n",
    "        eval_dataset=val_data,\n",
    "        args=transformers.TrainingArguments(\n",
    "            per_device_train_batch_size=micro_batch_size,\n",
    "            gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "            warmup_steps=100,\n",
    "            num_train_epochs=num_epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            fp16=True,\n",
    "            logging_steps=10,\n",
    "            optim=\"adamw_torch\",\n",
    "            evaluation_strategy=\"steps\" if val_set_size > 0 else \"no\",\n",
    "            save_strategy=\"steps\",\n",
    "            eval_steps=200 if val_set_size > 0 else None,\n",
    "            save_steps=200,\n",
    "            output_dir=out_dir,\n",
    "            save_total_limit=2,\n",
    "            #load_best_model_at_end=True if val_set_size > 0 else False,\n",
    "            #ddp_find_unused_parameters=False if ddp else None,\n",
    "            group_by_length=False,\n",
    "            #report_to=\"wandb\" if use_wandb else None,\n",
    "            #run_name=wandb_run_name if use_wandb else None,\n",
    "        ),\n",
    "    \n",
    "        # DataCollatorForSeq2Seq 로 지정.\n",
    "        data_collator=transformers.DataCollatorForSeq2Seq(\n",
    "            tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\", padding=True\n",
    "        ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ee478-979d-4880-b008-f136496e018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "model.config.use_cache = False\n",
    "\n",
    "old_state_dict = model.state_dict\n",
    "\n",
    "model.state_dict = (\n",
    "    lambda self, *_, **__: get_peft_model_state_dict(\n",
    "        self, old_state_dict()\n",
    "    )\n",
    ").__get__(model, type(model))\n",
    "\n",
    "\n",
    "#trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c65a23-b32f-46f2-9a6e-e8d4cb4750e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../data11/model/LLM/quantumaikr/KoreanLM-hf_LoRA_1/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80d9ad-7ce6-4bc5-9d7e-21e944ed4697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c125a1-4b4a-4bed-983c-152ee1a6304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임.\n",
    "# 참고 소스 : https://github.com/quantumaikr/KoreanLM/blob/main/generate.py\n",
    "#\n",
    "# 1. prompt는 모델마다 훈련 프롴프트가 다르므로, 해당 모델 prompt 로 입력하는게 보나 나은 출력이 나옴.\n",
    "# => KoAlpaca, KoAlpaca-Polyglot-5.8B prompt 예시=> '### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:'\n",
    "#\n",
    "# => open_llama_7b, KoreanLM, KoreanLM-hf prompt 예시(data/korean.json 파일 참조)\n",
    "#  \"아래는 작업을 설명하는 명령어와 추가 컨텍스트를 제공하는 입력이 짝을 이루는 예제입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {instruction} ### Input: {input} ### Response: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06364ff-603f-46e9-9ca7-37cbda99f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026ec814-d031-44a8-bf54-45450abf7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "#====================================================================\n",
    "# param 설정\n",
    "lora_weights:str = '../data11/model/LLM/quantumaikr/KoreanLM-hf_LoRA_1/'\n",
    "model_path:str ='../data11/model/LLM/quantumaikr/KoreanLM-hf/'\n",
    "\n",
    "uselora_weight = True # Lora 사용하는 경우 True\n",
    "load_8bit = True\n",
    "usekoalphaca = True   # Koalpaca 모델인 경우엔 True 해줘야함.\n",
    "#====================================================================\n",
    "\n",
    "# tokenizer 로딩\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    model_path,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "# 원본 모델 로딩\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    "    load_in_8bit=load_8bit,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "\n",
    "if uselora_weight:\n",
    "    # loRA 모델 로딩\n",
    "    model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            torch_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "\n",
    "if not load_8bit:\n",
    "    model.half()\n",
    "    \n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8c35db-0d10-4dd7-9ff7-c8a63d3db343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from typing import Union\n",
    "import json\n",
    "\n",
    "from transformers import GenerationConfig\n",
    "from myutils import GPU_info\n",
    "device = GPU_info()\n",
    "#print(device)\n",
    "\n",
    "# open_llama_7b, KoreanLM, KoreanLM-hf prompt 템플릿 경로\n",
    "template_file_name = \"./data/templates/korean.json\"\n",
    "\n",
    "# open_llama_7b, KoreanLM, KoreanLM-hf prompt 만드는 함수.\n",
    "def generate_prompt(\n",
    "        instruction: str,               # 설명\n",
    "        input: Union[None, str] = None, # 입력:옵션\n",
    "        label: Union[None, str] = None,\n",
    "    ) -> str:\n",
    "    \n",
    "    \n",
    "    if not osp.exists(template_file_name):\n",
    "        raise ValueError(f\"Can't read {template_file_name}\")\n",
    "            \n",
    "    with open(template_file_name) as fp:\n",
    "        template = json.load(fp)\n",
    "        \n",
    "    # returns the full prompt from instruction and optional input\n",
    "    # if a label (=response, =output) is provided, it's also appended.\n",
    "    if input:\n",
    "        res = template[\"prompt_input\"].format(instruction=instruction, input=input)\n",
    "    else:\n",
    "        res = template[\"prompt_no_input\"].format(instruction=instruction)\n",
    "        \n",
    "    if label:\n",
    "        res = f\"{res}{label}\"\n",
    "    \n",
    "    return res\n",
    "   \n",
    "'''\n",
    "대한민국의 남서쪽에 있는 섬으로 행정구역 상 광역자치단체인 제주특별자치도에 속한다. \n",
    "한국의 섬 중에서 가장 크고 인구가 많은 섬이기도 하며 면적은 1,833.2㎢이다. \n",
    "이는 대한민국 본토에서 가장 큰 기초자치단체인 홍천군(1,820.14㎢)보다 약간 크며, 제주도 다음 두 번째로 큰 섬인 거제도(379.5㎢)의 5배 정도 된다. \n",
    "인구는 약 70만 명, 세계 섬 크기 218위이다.\n",
    "제주도는 동아시아권 전체로 범위를 넓혀도 꽤 큰 섬에 속한다. \n",
    "6,000개가 넘는 섬이 있는 일본조차도 본토로 간주되는 혼슈, 홋카이도, 시코쿠, 규슈 4개 섬을 제외한 나머지 모든 섬이 제주도보다 작다.\n",
    "중국도 하이난 섬 한 곳만이 제주도보다 클 뿐이다.\n",
    "하와이에서도 최대 섬인 하와이 섬 다음으로 큰 섬인 마우이 섬이 제주도보다 약간 큰 정도이다. \n",
    "미국도 본토만 따지면 제주도보다 큰 섬은 롱아일랜드 뿐이다. \n",
    "프랑스도 본토에는 제주도보다 큰 섬이 코르시카 섬밖에 없고, 독일에서 가장 큰 섬인 뤼겐 섬은 제주도보다 작다. \n",
    "크기에 대한 직접적인 비교를 하자면 제주도의 동서 길이 약 73km, \n",
    "남 길이 약 31km를 대입하여 서울시청 기준 동서 길이로 인천광역시 서구 오류동 거첨도에서 출발하여 \n",
    "경기도 양평군 서종면에 도달하고 남북 길이로는 송추계곡에서 출발하여 관악산에 이르는 수준이다.\n",
    "\n",
    "질문 : 제주도 길이는 얼마?\n",
    "'''\n",
    "\n",
    "instruction ='''\n",
    "아래 내용을 가지고 질문에 답하세요\n",
    "\n",
    "학자금 지원\n",
    "목적 : 임직원 자녀의 학자금을 지급함으로써 임직원의 복지 향상, 근로의욕의 제고 및 장기근속 유도\n",
    "지원 기준\n",
    "임직원의 고등학교 재학중인 자녀 학자금 지원\n",
    "고등학교 교육비(입학금, 수업료, 학교운영지원금, 교과서)\n",
    "자녀수 제한은 없으며, 자녀당 연간 300만원 한도에서 지원\n",
    "지급신청 : 학자금의 신청은 아래 신청시기 해당월 이내에 다음 각호의 신청서류를 구비하여 해당부서장 결재 득 후 경영지원본부에 신청\n",
    "학자금 신청서 (별첨1)\n",
    "납입고지서 영수증 사본\n",
    "취학자녀의 재학증명서(최초 신청시 1회에 한함)\n",
    "가족관계증명원(최초 신청시 1회에 한함)\n",
    "신청시기 : 납부 후 1개월 이내\n",
    "지급제외 대상\n",
    "휴학, 퇴학 등의 사유로 학적이 변동된 경우\n",
    "타기관, 단체, 학교에서 장학금을 받는 경우 (일부를 받는 경우는 차액 지급)\n",
    "취학자녀의 부모가 임직원으로 동시에 지급대상일 경우 1인만 적용\n",
    "기타 : 해당금액은 과세대상임\n",
    "'''\n",
    "\n",
    "#instruction = \"엠파워 이지스-씨 제품 설명\"\n",
    "\n",
    "input_text = '''\n",
    "학자금은 얼마까지 지원가능 하며, 이때 신청 서류는 뭐가 있나요?\n",
    "'''\n",
    "# prompt 구성\n",
    "#========================================================================\n",
    "# koAlphaca일때 prompt 구성.\n",
    "# => \"### 질문: {input}\\n\\n### 맥락: {context}\\n\\n### 답변:\" 식으로 입력. \n",
    "# => '### 맥락' 은 이전 대화 맥락임.\n",
    "#=========================================================================\n",
    "if usekoalphaca:\n",
    "    input_text = instruction+'\\r\\n'+input_text\n",
    "    context = \"\" #이전 대화 맥락 입력\n",
    "    \n",
    "    prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "else:\n",
    "    prompt = generate_prompt(instruction=instruction, input=input_text)\n",
    "\n",
    "\n",
    "max_new_tokens = 512\n",
    "\n",
    "# config 설정\n",
    "generation_config = GenerationConfig(\n",
    "            temperature=0.5,\n",
    "            top_p=0.75,\n",
    "            top_k=40,\n",
    "            num_beams=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.pad_token_id\n",
    ")\n",
    "\n",
    "# 프롬프트 tokenizer \n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "#print(input_ids)\n",
    "\n",
    "# Without streaming\n",
    "# generate 처리\n",
    "with torch.no_grad():\n",
    "    generation_output = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generation_config,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )\n",
    "    \n",
    "# 출력\n",
    "#print(generation_output)\n",
    "#print()\n",
    "\n",
    "s = generation_output.sequences[0]\n",
    "output = tokenizer.decode(s)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b063d8-a805-4b35-9d42-c1cfda27ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 평가 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109bb17-8ebf-4c4e-b0e4-7eb0e978f0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
