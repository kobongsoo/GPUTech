{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf1110b-470b-4106-aa7b-3bb9954ea230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================================================\n",
    "# loRA 파인튜닝 예제1\n",
    "# - bigscience/bloom-3b 모델을 이용하여 PEFT(Parameter-Efficient Fine-tuning)->LoRA(Low-Rank Adaptation) 기법으로 파인튜닝하는 예시\n",
    "# - 설명 : https://medium.com/@jeremyarancio/fine-tune-an-llm-on-your-personal-data-create-a-the-lord-of-the-rings-storyteller-6826dd614fa9\n",
    "#\n",
    "# 참조 DOC : https://github.com/jeremyarancio/llm-rpg/tree/main\n",
    "# 참고소스: https://github.com/jeremyarancio/llm-rpg/blob/main/llm/training.py\n",
    "#\n",
    "# 참고소스: https://github.com/Beomi/KoAlpaca\n",
    "# Huggingface\n",
    "# 참조 DOC: https://huggingface.co/docs/peft/index\n",
    "# 참조소스: https://github.com/huggingface/peft\n",
    "#\n",
    "# package 설치 \n",
    "# peft: pip install peft\n",
    "# load_in_8bit : pip install -i https://test.pypi.org/simple/ bitsandbytes-cudaXXX  (XXX는 CUDA version (e.g. 11.6 = 116))\n",
    "# transfomers 4.27.1 이상으로 업데이트 : pip install -U transformers[pytorch]\n",
    "# dispatch_model() got an unexpected keyword argument 'offload_index' 오류 => accelerate 업데이트 : pip install -U accelerate\n",
    "# module 'bitsandbytes.nn' has no attribute 'Linear8bitLt' => pip install bitsandbytes==0.37.2\n",
    "# 'MatmulLtState' object has no attribute 'memory_efficient_backward' 오류 => bitsandbytes 버전 0.37.2 설치 : pip install bitsandbytes==0.37.2\n",
    "#====================================================================================================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, RandomSampler, SequentialSampler \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast, AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import time\n",
    "from myutils import GPU_info, seed_everything, mlogging, SaveBERTModel, AccuracyForMLM\n",
    "\n",
    "# 입력\n",
    "#model_path='../data11/model/LLM/bigscience/bloomz-7b1-mt/'\n",
    "model_path='beomi/KoAlpaca'\n",
    "\n",
    "device = GPU_info()\n",
    "print(device)\n",
    "\n",
    "#seed 설정\n",
    "seed_everything(111)\n",
    "\n",
    "#logging 설정\n",
    "logger =  mlogging(loggername=\"KoAlpaca\", logfilename=\"../log/KoAlpaca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613ca93-9f78-40a6-9486-bebc22988935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3790b943-6d47-496b-ae50-e96fb40d2cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "### 전체모델 저장\n",
    "TMP_OUT_PATH = '../data11/model/LLM/beomi/KoAlpaca/'\n",
    "os.makedirs(TMP_OUT_PATH, exist_ok=True)\n",
    "#torch.save(model, OUTPATH + 'pytorch_model.bin') \n",
    "# save_pretrained 로 저장하면 config.json, pytorch_model.bin 2개의 파일이 생성됨\n",
    "model.save_pretrained(TMP_OUT_PATH)\n",
    "\n",
    "# tokeinizer 파일 저장(vocab)\n",
    "VOCAB_PATH = TMP_OUT_PATH\n",
    "tokenizer.save_pretrained(VOCAB_PATH)\n",
    "print(f'==> save_model : {TMP_OUT_PATH}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154b5d0-18f0-4bea-82f0-6c2415c4c6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer 로딩 \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06240c79-1531-4eab-a43f-5c608cbaabbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 하고, embedding size를 tokenizer 사이즈만큼 조정\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch import float32, nn, exp\n",
    "\n",
    "\n",
    "# 참고 소스 : https://github.com/jeremyarancio/llm-rpg/blob/main/llm/training_utils.py\n",
    "class CastOutputToFloat(nn.Sequential):\n",
    "    def forward(self, x): \n",
    "        return super().forward(x).to(float32)\n",
    "\n",
    "    \n",
    "def prepare_model(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False  # freeze the model - train adapters later\n",
    "        if param.ndim == 1:\n",
    "            # cast the small parameters (e.g. layernorm) to fp32 for stability\n",
    "            param.data = param.data.to(float32)\n",
    "    model.gradient_checkpointing_enable()  # reduce number of stored activations\n",
    "    model.enable_input_require_grads()\n",
    "    model.lm_head = CastOutputToFloat(model.lm_head)\n",
    "    return model\n",
    "\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    return f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "\n",
    "'''\n",
    "#model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_path, revision='KoGPT6B-ryan1.5b-float16',  # or float32 version: revision=KoGPT6B-ryan1.5b\n",
    "  pad_token_id=tokenizer.eos_token_id,\n",
    "  torch_dtype='auto', low_cpu_mem_usage=True\n",
    ").to(device=device, non_blocking=True)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "model.to(device)\n",
    "'''\n",
    "\n",
    "# 모델 로딩\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, return_dict=True, load_in_8bit=True, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918bd536-13e0-4dc5-aeb8-81e475013aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44db266-3ae7-4a58-a41e-b6212ca598b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lora 와 연동\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "model = prepare_model(model)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CLM or Seq2Seq\n",
    ")\n",
    "\n",
    "#peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=16, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22fc288-ce75-4db8-aea8-16465ad6afb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc32f8-d7ce-4f57-ba92-2c3454335275",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac49472-e1c8-4efb-af59-4dd405f64a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 셋을 생성한다.\n",
    "# => 문장뒤에 </s> 추가.\n",
    "from datasets import Dataset\n",
    "#corpus_path = \"../data11/korpora/kowiki_20190620/wiki_20190620_small.txt\"\n",
    "corpus_path = \"../data11/ai_hub/vs1/IT.과학.txt\"\n",
    "texts = \"\"\n",
    "with open(corpus_path, 'r') as f:\n",
    "    for line in f:\n",
    "        if len(line) > 1:\n",
    "            texts += line\n",
    "        \n",
    "    # 문장들에서 '\\n'(띄어쓰기) -> '.</s>'로 치환함.\n",
    "    texts = texts.replace(\"\\n\", \".\" + tokenizer.eos_token)\n",
    " \n",
    "# 데이터 셋 생성\n",
    "dataset = Dataset.from_dict({'text': [texts]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32de01ef-69cf-4632-add9-29b5c960123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Mapping\n",
    "\n",
    "# tokenizer 처리 함수\n",
    "def tokenize(element: Mapping, tokenizer: Callable, context_length: int) -> str:\n",
    "    inputs = tokenizer(element['text'], truncation=True, return_overflowing_tokens=True, \n",
    "                       return_length=True, max_length=context_length)\n",
    "    inputs_batch = []\n",
    "    for length, input_ids in zip(inputs['length'], inputs['input_ids']):\n",
    "        if length == context_length: # We drop the last input_ids that are shorter than max_length\n",
    "            inputs_batch.append(input_ids)\n",
    "    return {\"input_ids\": inputs_batch}\n",
    "\n",
    "# tokenizer dataset 생성\n",
    "context_length = 2048  # 문장 길이(모델의 허용할수 있는 최대 길이 설정)\n",
    "test_size = 0.001\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True, fn_kwargs={'tokenizer': tokenizer, 'context_length': context_length},\n",
    "                                         remove_columns=dataset.column_names)\n",
    "\n",
    "tokenized_dataset_dict = tokenized_dataset.train_test_split(test_size=test_size, shuffle=True)\n",
    "\n",
    "print(f'len(tokenized_dataset):{len(tokenized_dataset)}')\n",
    "print(f'len(tokenized_dataset_dict):{len(tokenized_dataset_dict)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1099a1eb-9f38-40d0-9813-3d571428548f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 param 설정 \n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "output_dir = './output'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.1,\n",
    "    learning_rate=3e-4, \n",
    "    fp16=True,\n",
    "    logging_steps=1000,    # 훈련시, 로깅할 step 수 (크면 10000번 정도하고, 작으면 100번정도)\n",
    "    output_dir=output_dir\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_dataset_dict['train'],\n",
    "    args=training_args,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec23cf9-9cb7-4a56-8df7-6854c10bf02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 시작\n",
    "model.config.use_cache = False  # silence warnings\n",
    "trainer.train()\n",
    "model.config.use_cache = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8291f86-a0ce-4b56-9ca6-db731b377146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 완료후 저장\n",
    "import os\n",
    "\n",
    "OUT_PATH = '../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B-LoRA-qa-1/'\n",
    "os.makedirs(OUT_PATH, exist_ok=True) # 폴더 생성\n",
    "\n",
    "# adapter_config.json 와 adapter_config.bin 파일 생성. \n",
    "model.save_pretrained(OUT_PATH)\n",
    " \n",
    "# 실제 모델 파일 (pytorch_model.bin, training_args.bin)\n",
    "trainer.save_state()  # 모델이 모든 가중치, 학습률, 알고리즘등을 저장.\n",
    "trainer.save_model(output_dir=OUT_PATH) # 모델이 가중치만 저장\n",
    "\n",
    "# pytorch_model.bin 이름을 adapter_model.bin 변경.\n",
    "# => 그래야 huggingface 를 이용해서 불러올수 있음.\n",
    "if os.path.exists(os.path.join(OUT_PATH, 'adapter_model.bin')):\n",
    "    os.remove(os.path.join(OUT_PATH, 'adapter_model.bin'))\n",
    "    print(f'remove adapter_model.bin')\n",
    "\n",
    "if os.path.exists(os.path.join(OUT_PATH, 'pytorch_model.bin')):\n",
    "    os.rename(os.path.join(OUT_PATH, 'pytorch_model.bin'), os.path.join(OUT_PATH, 'adapter_model.bin'))\n",
    "    print(f'rename pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c125a1-4b4a-4bed-983c-152ee1a6304c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06364ff-603f-46e9-9ca7-37cbda99f23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기서부터는 저장된 모델 평가하는 코드임."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7576834b-126a-4209-b0c7-90da5d76a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Import the model\n",
    "hf_repo = '../data11/model/LLM/quantumaikr/KoreanLM_LoRA_1/'\n",
    "\n",
    "config = PeftConfig.from_pretrained(hf_repo)\n",
    "config.base_model_name_or_path = '../data11/model/LLM/quantumaikr/KoreanLM/'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
    "\n",
    "# Lora model 불러오기\n",
    "model = PeftModel.from_pretrained(model, hf_repo)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf28c0e3-0e6a-4b63-91eb-eca4b2695c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import GPU_info\n",
    "device = GPU_info()\n",
    "\n",
    "'''\n",
    "수학은 기원전 600년 경에 살았던 탈레스로부터 시작됐다.\n",
    "하지만 탈레스가 태어나기 전에도 수학을 연구한 사람이 있을 수도 있기 때문에, 인류의 역사와 더불어 시작되었다고 할 수 있다.\n",
    "교역•분배•과세 등의 인류의 사회 생활에 필요한 모든 계산을 수학이 담당해 왔고, 농경 생활에 필수적인 천문 관측과 달력의 제정, 토지의 측량 또한 수학이 직접적으로 관여한 분야이다.\n",
    "고대 수학을 크게 발전시킨 나라로는 이집트, 인도, 그리스, 중국 등이 있다.\n",
    "그 중에서도 그리스는 처음으로 수학의 방정식에서 변수를 문자로 쓴 나라이다.\n",
    "\n",
    "위 내용을 바탕으로 아래 질문에 대해 답변해 주세요\n",
    "질문 : 고대 수학을 가장 크게 발전시킨 곳은?\n",
    "'''\n",
    "prompt =\"모코엠시스 라는 IT 회사에 대해 간략히 설명해줘\"\n",
    "\n",
    "## Generate\n",
    "max_new_tokens = 200\n",
    "temperature = 0.5\n",
    "do_sample = False\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "tokens = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "print(tokenizer.decode(tokens[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b063d8-a805-4b35-9d42-c1cfda27ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 평가 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6275bb2-f8d0-419d-9b70-496bc24d9a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 모델 로딩\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "# Import the model\n",
    "model_path = '../data11/model/LLM/quantumaikr/KoreanLM/'\n",
    "\n",
    "model1 = AutoModelForCausalLM.from_pretrained(model_path, return_dict=True, load_in_8bit=True, device_map='auto')\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "model1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8641133-bc3d-4982-ba5c-b136034c8aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils import GPU_info\n",
    "device = GPU_info()\n",
    "\n",
    "prompt =\"모코엠시스 라는 IT 회사에 대해 간략히 설명해줘\"\n",
    "\n",
    "## Generate\n",
    "max_new_tokens = 200\n",
    "temperature = 0.5\n",
    "do_sample = False\n",
    "\n",
    "# Generate text\n",
    "inputs = tokenizer1(prompt, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"].to(device)\n",
    "\n",
    "tokens1 = model1.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            eos_token_id=tokenizer1.eos_token_id,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "print(tokenizer1.decode(tokens1[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c109bb17-8ebf-4c4e-b0e4-7eb0e978f0d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
