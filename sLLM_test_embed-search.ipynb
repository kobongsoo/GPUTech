{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820aa697-ef0d-42e5-8f0f-04852a14e504",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------------\n",
    "# BERT와 sLLM 모델을 이용한 질의응답 서비스 구축 예\n",
    "# - sLLM_test_embed.ipynb에서 임베딩된 문서들을 이용하여 프롬프트 생성 및 입력하는 예시임.\n",
    "#\n",
    "# 질의 응답 시스템 과정\n",
    "# 문서들 전처리 : \n",
    "#    단락별루 분할(\\n\\n) - 불용어 제거 -문장별루 분할.\n",
    "# 임베딩 : \n",
    "#    kpf-sbert-v1.1로  문장 평균 임베딩벡터 구함 - es에 문장별루 단락text와 평균벡터 저장.\n",
    "# 프롬프트생성 및 입력 : \n",
    "#   검색어 입력(회사:과장일때 휴가 일수는 얼마?)-bert로 임베딩 검색(*스코어가 0.6이상인 경우 체택)-sLLM에 검색된 단락 text를 문맥으로 해서 prompt 구성\n",
    "#   sLLM에 prompt 입력-응답 결과 출력\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from myutils import seed_everything, GPU_info, mlogging\n",
    "\n",
    "LOGGER = mlogging(loggername=\"sllm-test\", logfilename='../log/sll-test.txt') # 로그\n",
    "\n",
    "# param-----------------------------------\n",
    "SEED = 111\n",
    "seed_everything(SEED)\n",
    "DEVICE = GPU_info() # GPU 혹은 CPU\n",
    "OUT_DIMENSION = 0   # 출력 dimension 128 혹은 0(768)\n",
    "EMBEDDING_METHOD=1 # 1=평균\n",
    "NUM_CLUSTERS=10\n",
    "NUM_CLUSTERS_VARIABLE=False\n",
    "   \n",
    "BATCH_SIZE=20\n",
    "FLOAT_TYPE = 'float16'\n",
    "\n",
    "# ES 접속\n",
    "ES_URL = 'http://10.10.4.10:9200/'             # elasticsearch 접속 url\n",
    "ES_INDEX_NAME = 'mpower_doc_768d_f16_1'            # elasticsearch 인덱스명\n",
    "ES_INDEX_FILE = './data/mpower10u_768d_1.json' # 인덱스 생성 파일\n",
    "SEARCH_SIZE = 3                                # 검색 계수\n",
    "MIN_SCORE = 1.3                                # 검색 1.4 스코어 이하면 제거\n",
    "\n",
    "# 임베딩 모델 param\n",
    "MODEL_PATH = '../data11/model/kpf-sbert-v1.1' #'../data11/model/kpf-sbert-128d-v1'\n",
    "POLLING_MODE = 'mean'\n",
    "\n",
    "# sLLM 모델 param\n",
    "LLM_MODEL_TYPE = 0      # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델.\n",
    "lora_weights:str = '../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B-LoRA-qa-moco-3/'      # lora weight 경로\n",
    "llm_model_path:str ='../data11/model/LLM/beomi/KoAlpaca-Polyglot-5.8B/'     # llm 모델경로(KoAlpaca-Polyglot-5.8B 모델이 가장 속도가 빠르고, 잘 응답하는것 같음.)\n",
    "uselora_weight = True # Lora 사용하는 경우 True\n",
    "load_8bit = True       # 8bit 로딩 \n",
    "\n",
    "# prompt_template(모델에 따라 변경)\n",
    "PROMPT_DICT = {\n",
    "    #\"prompt_context\":(\"아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 질문: {query}\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"질문에 대해 간략히 답변해 주세요\\n\\n### 질문: {query}\\n\\n### 답변:\")\n",
    "     \n",
    "    #\"prompt_context\":(\"### 질문: {query}\\n\\n아래 내용을 가지고 질문에 대해 간략히 답변해 주세요\\n\\n### 내용: {context}\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"### 질문: {query}\\n\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"### 질문: {context}\\n\\n위 내용을 간략히 요약해 주세요\\n\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"### 질문: {context}\\n\\n위 내용을 간략히 요약해 주세요\\n\\n### 답변:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"{context}\\n위 내용을 간략히 요약해 주세요\\n\\n\"),\n",
    "    #\"prompt_no_context\":(\"{context}\\n위 내용을 간략히 요약해 주세요\\n\\n\"),\n",
    "    \n",
    "    #\"prompt_context\":(\"다음 내용을 파악하여 질문에 대해 간략한 답변을 작성하세요.\\n### 내용: {context}\\n### 질문: {query}\\n### 답변:\"),\n",
    "    #\"prompt_no_context\":(\"질문에 대해 간략한 답변을 작성하세요.\\n### 질문: {query} ### 답변:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"### 지시: [질의]에 가장 적합한 [응답]을, 제시된 [문단]에서 찾아서 간단한 문장으로 답변해주세요.\\n\\n### [질의]: {query}\\n\\n### [문단]: {context}\\n\\n### [응답]:\"),\n",
    "    #\"prompt_no_context\":(\"### 지시: 질의에 대해, 간단한 문장으로 응답해 주세요 ### [질의]: {query}\\n\\n### [응답]:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"### 지시: [질의]에 가장 적합한 [응답]을, 제시된 [문단]을 바탕으로 문장으로 작성해 주세요.\\n\\n### [질의]: {query}\\n\\n### [문단]: {context}\\n\\n### [응답]:\"),\n",
    "    #\"prompt_no_context\":(\"### 지시: 질의에 대해, 자세하게 문장으로 작성해 주세요\\n\\n### [질의]: {query}\\n\\n### [응답]:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"###지시: 문단에서 질의에 대해 가장 적합한 내용을 찾아 응답 문장을 만들어 주세요.\\n\\n###[문단]: {context}\\n\\n###[질의]: {query}\\n\\n###[응답]:\"),\n",
    "    #\"prompt_no_context\":(\"###지시: 질의에 대해 자세하게 응답 문장을 만들어 주세요.\\n\\n###[질의]: {query}\\n\\n###[응답]:\")\n",
    "    \n",
    "    \"prompt_context\":(\"###지시: 문단에서 질의에 대해 가장 적합한 내용을 찾아 응답 문장을 만들어 주세요.\\n\\n###문단: {context}\\n\\n###질의: {query}\\n\\n###응답:\"),\n",
    "    \"prompt_no_context\":(\"###지시: 질의에 대해 자세하게 응답 문장을 만들어 주세요.\\n\\n###질의: {query}\\n\\n###응답:\")\n",
    "}\n",
    "#-------------------------------------------\n",
    "\n",
    "# prompt 테스트 \n",
    "print(f'\\n\\nprompt 테스트----------------------\\n')\n",
    "query='제주도의 크기는 얼마?'\n",
    "#context = ''\n",
    "context = '제주도는 최남단에 있는 섬으로, 길이는 40km가 되는 대한민국에서 가장큰 섬이다.'\n",
    "if context:\n",
    "    prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "else:\n",
    "    prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "    \n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dc695f-b2ac-4802-998f-37987e263390",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import re\n",
    "\n",
    "response = \"\"\"\n",
    "### 지시: 답변\\n\\n### [문단]: [출장 여비 규정]\\n4.1. 일수 계산 시, 기산은 오전 0시부터로 하며 1일 미만의 단수는 1일로 계산한다.\\n\\n### [질의]: 1일 출장시 숙박비는 얼마?\\n\\n### [응답]: 1일 출장시 숙박비는 1일당 실비로 지원합니다.\n",
    "\"\"\"\n",
    "\n",
    "context:str = \"\"\n",
    "if response:\n",
    "    answers = response.split(\"###[응답]:\")\n",
    "        \n",
    "    # [문단]부터 [질의]까지 추출\n",
    "    pattern = r\"\\[문단\\]: (.+?)(?=\\n\\n### \\[질의\\])\"\n",
    "    context = re.search(pattern, response, re.DOTALL).group(1)\n",
    "\n",
    "print(context)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b593a7b-4d34-4c08-b625-d5a7cf45d2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# 임베딩 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "from tqdm.notebook import tqdm\n",
    "from myutils import embed_text, bi_encoder, mpower_index_batch\n",
    "\n",
    "TOKENIZER1, BI_ENCODER1 = bi_encoder(model_path=MODEL_PATH, max_seq_len=512, do_lower_case=True, pooling_mode=POLLING_MODE, out_dimension=OUT_DIMENSION, device=DEVICE)\n",
    "\n",
    "print(BI_ENCODER1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990059ea-bdc3-4328-9e0d-c2653cee6b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# sLLM 모델 로딩\n",
    "#----------------------------------------------------------------------\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "import time\n",
    "\n",
    "if LLM_MODEL_TYPE == 0:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델. == True:\n",
    "    start_time = time.time()\n",
    "\n",
    "    # tokenizer 로딩\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(llm_model_path)\n",
    "\n",
    "    # 원본 모델 로딩\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(llm_model_path, load_in_8bit=load_8bit, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "    if uselora_weight:\n",
    "        model = PeftModel.from_pretrained(model, lora_weights, torch_dtype=torch.float16) # loRA 모델 로딩\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"time: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    print(model)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ed00d-724c-49c0-8504-a1cd1a5834ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ES 관련\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch.helpers import bulk\n",
    "\n",
    "#--------------------------------------------\n",
    "# 조건에 맞게 임베딩 처리하는 함수 \n",
    "#--------------------------------------------\n",
    "def embedding(paragraphs:list)->list:\n",
    "    # 한 문단에 대한 40개 문장 배열들을 한꺼번에 임베딩 처리함\n",
    "    embeddings = embed_text(model=BI_ENCODER1, paragraphs=paragraphs, return_tensor=False).astype(FLOAT_TYPE)    \n",
    "    return embeddings\n",
    "\n",
    "#------------------------------------------------------------------------\n",
    "# ES임베딩 쿼리 벡터\n",
    "# -쿼리 임베딩 벡터를 구하고, es에 접속해서 rfile_text 뽑아냄\n",
    "#------------------------------------------------------------------------\n",
    "def es_embed_query(query:str):\n",
    "    assert query, f'query is empty'\n",
    "    \n",
    "    # 1.elasticsearch 접속\n",
    "    es = Elasticsearch(ES_URL)  \n",
    "\n",
    "    # 임베딩 구함.\n",
    "    start_time = time.time()\n",
    "    embed_query = embedding([query])[0]\n",
    "    \n",
    "    #print(len(embed_query))\n",
    "    \n",
    "    # 쿼리 구성\n",
    "    script_query = {\n",
    "        \"script_score\":{\n",
    "            \"query\":{\n",
    "                \"match_all\": {}},\n",
    "            \"script\":{\n",
    "                \"source\": \"cosineSimilarity(params.query_vector, doc['vector1']) + 1.0\",  # 뒤에 1.0 은 코사인유사도 측정된 값 + 1.0을 더해준 출력이 나옴\n",
    "                \"params\": {\"query_vector\": embed_query}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    #print(script_query)\n",
    "\n",
    "    # 실제 ES로 검색 쿼리 날림\n",
    "    start_search_time = time.time()\n",
    "    response = es.search(\n",
    "        index=ES_INDEX_NAME,\n",
    "        body={\n",
    "            \"size\": SEARCH_SIZE,\n",
    "            \"query\": script_query,\n",
    "            \"_source\":{\"includes\": [\"rfile_name\", \"rfile_text\"]}\n",
    "        }\n",
    "    )\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*ES 검색시간: {:.2f} ms\".format(end_time * 1000)) \n",
    "\n",
    "    count = 0\n",
    "    docs = []\n",
    "    for hit in response[\"hits\"][\"hits\"]: \n",
    "        doc = {}  #dict 선언\n",
    "        doc['rfile_name'] = hit[\"_source\"][\"rfile_name\"]      # contextid 담음\n",
    "        doc['rfile_text'] = hit[\"_source\"][\"rfile_text\"]      # text 담음.\n",
    "        doc['score'] = hit[\"_score\"]\n",
    "        docs.append(doc) \n",
    "            \n",
    "    return docs\n",
    "#------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219e7066-b17b-4eb1-8a44-b3332749e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# prompt 생성\n",
    "#--------------------------------------------\n",
    "def make_prompt(docs, query)->str:\n",
    "     # prompt 구성\n",
    "    context:str = ''\n",
    "\n",
    "    for doc in docs:\n",
    "        score = doc['score']\n",
    "        if score > MIN_SCORE:\n",
    "            rfile_text = doc['rfile_text']\n",
    "            if rfile_text:\n",
    "                context += rfile_text + '\\n\\n'\n",
    "                \n",
    "    if context:\n",
    "        prompt = PROMPT_DICT['prompt_context'].format(query=query, context=context)\n",
    "    else:\n",
    "        prompt = PROMPT_DICT['prompt_no_context'].format(query=query)\n",
    "                \n",
    "    # KoAlpaca 프롬프트\n",
    "    #prompt = f\"### 질문: {query}\\n질문에 대해 아래 내용을 바탕으로 간략히 답변해 주세요\\n\\n### 문맥: {context}\\n\\n### 답변:\" if context else f\"### 질문: {query}\\n질문에 대해 간략히 답변해 주세요\\n\\n### 답변:\"\n",
    "    \n",
    "    # llama 프롬프트\n",
    "    #prompt = f\"아래는 작업을 설명하는 명령어입니다. 요청을 적절히 완료하는 응답을 작성하세요. ### Instruction: {context}\\n{query} ### Response:\"\n",
    "\n",
    "    #print(prompt)\n",
    "    #print()\n",
    "    return prompt\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a0422-be57-4953-a614-3cca5d907093",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "#-----------------------------------------\n",
    "# sLLM 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "def generate_text_sLLM(prompt):\n",
    "    \n",
    "    max_new_tokens = 256\n",
    "    eos_str = tokenizer.decode(tokenizer.eos_token_id)\n",
    "    start_time = time.time()\n",
    "    #print(f'eos_str:{eos_str}')\n",
    "    \n",
    "    #prompt = query\n",
    "    #prompt = f\"### 질문: {input_text}\\n\\n### 맥락: {context}\\n\\n### 답변:\" if context else f\"### 질문: {input_text}\\n\\n### 답변:\"\n",
    "    #prompt = f\"### 질문 : 간략히 답변해줘.{query}\\r\\n###답변:\"\n",
    "    #prompt = f\"### 질문: {query}\\n\\n### 답변:\"\n",
    "    #print(prompt)\n",
    "\n",
    "    # config 설정\n",
    "    generation_config = GenerationConfig(\n",
    "        temperature=0.5,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=1,\n",
    "        bos_token_id=tokenizer.bos_token_id,  # 시작토큰 \n",
    "        eos_token_id=tokenizer.eos_token_id,  # end 토큰\n",
    "        pad_token_id=tokenizer.pad_token_id   # padding 토큰\n",
    "    )\n",
    "\n",
    "    # 프롬프트 tokenizer \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    #print(input_ids)\n",
    "       \n",
    "    # Without streaming\n",
    "    # generate 처리\n",
    "    with torch.no_grad():\n",
    "        generation_output = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            generation_config=generation_config,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=False,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "        )\n",
    "\n",
    "    # 출력\n",
    "    s = generation_output.sequences[0]\n",
    "    output = tokenizer.decode(s)\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    print(\"*Text생성시간: {:.2f} ms\\n\".format(end_time * 1000)) \n",
    "    #print(output.replace(eos_str, ''))\n",
    "    #print()\n",
    "    return output.replace(eos_str, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b5925-e803-4ca5-ad24-4a8862e97641",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------\n",
    "# GPT를 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "import openai\n",
    "\n",
    "# key 지정\n",
    "openai.api_key = \"sk-xxx\"\n",
    "\n",
    "# 모델 - GPT 3.5 Turbo 지정\n",
    "# => 모델 목록은 : https://platform.openai.com/docs/models/gpt-4 참조\n",
    "gpt_model = \"gpt-3.5-turbo\"#\"gpt-4\"#\"gpt-3.5-turbo\" #gpt-4-0314\n",
    "'''\n",
    "ROLE_PROMPT = \"아래 내용을 요약해서 질문에 알맞게 간략히 답변해주세요\"\n",
    "\n",
    "# 메시지 설정\n",
    "MESSAGES = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helful assistant.\"},\n",
    "            #{\"role\": \"system\", \"content\": \"질문에 대해 요약해줘\"},\n",
    "            #{\"role\": \"user\", \"content\" : \"How are you?\"},\n",
    "            #{\"role\": \"assistant\", \"content\" : \"I am doing well\"},\n",
    "            {\"role\": \"user\", \"content\": ROLE_PROMPT}\n",
    "        ]\n",
    "'''\n",
    "MESSAGES:list = []\n",
    "#-----------------------------------------\n",
    "# GPT를 이용한 text 생성\n",
    "#-----------------------------------------\n",
    "def generate_text_GPT(prompt, messages):\n",
    "    \n",
    "    print(f'len(messages):{len(messages)}') \n",
    "    print()\n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # *** gpt에 메시지는 계속 대화 내용이 유지가 되므로, 비용이 발생함.\n",
    "    # 따라서 최근 2개 대화만 유지함.\n",
    "    #if len(messages) >= 2:\n",
    "    #    messages = messages[len(messages)-2:]  # 최근 2개의 대화만 가져오기\n",
    "    messages = []  # 무조건 최근대화 초기화\n",
    "     #-------------------------------------------------------\n",
    "        \n",
    "    # 사용자 메시지 추가\n",
    "    messages.append( {\"role\": \"user\", \"content\": prompt})\n",
    "    print(messages)\n",
    "\n",
    "    # ChatGPT-API 호출하기\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages,\n",
    "        max_tokens=500, # 토큰 수 \n",
    "        temperature=1,  # temperature 0~2 범위 : 작을수록 정형화된 답변, 클수록 유연한 답변(2는 엉뚱한 답변을 하므로, 1.5정도가 좋은것 같음=기본값은=1)\n",
    "        top_p=0.1 # 기본값은 1 (0.1이라고 하면 10% 토큰들에서 출력 토큰들을 선택한다는 의미)\n",
    "    )\n",
    "\n",
    "    print(response)\n",
    "    print()\n",
    "    answer = response['choices'][0]['message']['content']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fda5791-3cae-488b-ae49-725529c59b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------\n",
    "# 구글 bard를 이용한 text 생성\n",
    "#\n",
    "# 세션키를 이용하여 구글 bard 테스트 예제\n",
    "# 출처 : https://github.com/dsdanielpark/Bard-API\n",
    "#\n",
    "# token 값얻기\n",
    "# https://bard.google.com/ 방문\n",
    "# 콘솔용 F12\n",
    "# 세션: 애플리케이션 → 쿠키 → 쿠키 값을 복사합니다 __Secure-1PSID.\n",
    "# -> 참고로 반드시 뒤에 .으로 끝나고 .포함해서 길이가 72자임.\n",
    "#------------------------------------------------------------------\n",
    "\n",
    "from bardapi import Bard\n",
    "\n",
    "token = 'xxx' # bard 토큰 입력\n",
    "\n",
    "def generate_text_BARD(prompt):\n",
    "    \n",
    "    bard = Bard(token=token,timeout=30) # Set timeout in seconds\n",
    "    answer = bard.get_answer(prompt)['content']\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf69ccc9-5870-4d7a-b047-8f128dfbc4e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 쿼리 입력후 테스트\n",
    "def run_query_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            handle_query()\n",
    "        except KeyboardInterrupt:\n",
    "            return\n",
    "\n",
    "def handle_query():\n",
    "    # 쿼리는 일반 쿼리인 경우 '일반##제주도 면적은 얼마?'\n",
    "    #query = '일반##제주도 면적은 얼마?' \n",
    "    query = input(\"질문을 입력하세요\")\n",
    "    \n",
    "    query_split = query.split('##')\n",
    "    prefix = query_split[0]\n",
    "    #print(f'prefix: {prefix}')\n",
    "    \n",
    "    if prefix == '@':  # 일반쿼리일때는 @## prefix 입력후 질문입력함. \n",
    "        query1 = query_split[1]\n",
    "        prompt = make_prompt(docs='', query=query1)\n",
    "    else:\n",
    "        query1 = query\n",
    "        docs = es_embed_query(query1)\n",
    "        print(docs)\n",
    "        print()\n",
    "        prompt = make_prompt(docs=docs, query=query1)\n",
    "        \n",
    "    if LLM_MODEL_TYPE == 0:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델\n",
    "        print(generate_text_sLLM(prompt))\n",
    "    elif LLM_MODEL_TYPE == 1:     # 0=아래 지정한 sLLM 모델, 1=GPT 모델, 2=BARD 모델:\n",
    "        print(generate_text_GPT(prompt=prompt, messages=MESSAGES))\n",
    "    else:\n",
    "        print(generate_text_BARD(prompt))\n",
    "        \n",
    "    print()\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0988be-a7f5-4ada-8d63-5184c7d80eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROMPT_DICT = {   \n",
    "    #\"prompt_context\":(\"###지시: 문단에서 질의에 대해 가장 적합한 내용들을 찾고, 찾은 내용을 질의에 적합하게 요약해서 답변해 주세요\\n\\n###[문단]: {context}\\n\\n###[질의]: {query}\\n\\n###[응답]:\"),\n",
    "    #\"prompt_no_context\":(\"###지시: 질의에 대해, 자세하게 문장으로 작성해 주세요\\n\\n### [질의]: {query}\\n\\n###[응답]:\")\n",
    "    \n",
    "    #\"prompt_context\":(\"###지시: 문단에서 질의에 대해 가장 적합한 내용을 찾아, 응답 문장을 만들어 주세요.\\n\\n###[문단]: {context}\\n\\n###[질의]: {query}\\n\\n###[응답]:\"),\n",
    "    #\"prompt_no_context\":(\"###지시: 질의에 대해, 자세하게 응답 문장을 만들어 주세요.\\n\\n###[질의]: {query}\\n\\n###[응답]:\")\n",
    "    \n",
    "    \"prompt_context\":(\"###지시: 문단에서 질의에 대해 가장 적합한 내용을 찾아 응답 문장을 만들어 주세요.\\n\\n###문단: {context}\\n\\n###질의: {query}\\n\\n###응답:\"),\n",
    "    \"prompt_no_context\":(\"###지시: 질의에 대해 자세하게 응답 문장을 만들어 주세요.\\n\\n###질의: {query}\\n\\n###응답:\")\n",
    "}\n",
    "\n",
    "run_query_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39157fe-2ec6-488a-be20-dc46e095b8a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
